---
title: "Measuring implicit bias in explicitly unbiased large language models"
description: "<div class='reference-container'>
                <div class='info'>
                  <a>Bai, X., Wang, A., Sucholutsky, I., & Griffiths, T. L. (under review)</a> 
                </div>
                <div class='materials'>
                <a href='https://arxiv.org/abs/2402.04105' class='link material'>[preprint]</a>
                </div>
              </div>"
author: ["Xuechunzi Bai","Angelina Wang", "Ilia Sucholutsky","Thomas L. Griffiths"]
date: 2024-05-24
categories: ["stereotype", "language model"] 
image: "cover.jpg" 
title-block-banner: false 
format: 
  html: 
    page-layout: full 
    css:
      - ../../../css/research-page.css
---

------------------------------------------------------------------------

::: page-container
::: summary-card
<div>

Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two new measures of bias: LLM Implicit Bias, a prompt-based method for revealing implicit bias; and LLM Decision Bias, a strategy to detect subtle discrimination in decision-making tasks. Both measures are based on psychological research: LLM Implicit Bias adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Decision Bias operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). Our prompt-based LLM Implicit Bias measure correlates with existing language model embedding-based bias methods, but better predicts downstream behaviors measured by LLM Decision Bias. These new prompt-based measures draw from psychology's long history of research into measuring stereotype biases based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks.

</p>

</div>
:::

::: main-content
# Summary

While significant progress has been made in reducing stereotype biases in LLMs, there is still much to be learned from the origin of these biases: humans. Despite century-long efforts to reduce prejudice and discrimination in human society, humans have not eliminated bias but rather learned to transform blatant stereotypes into harder-to-see forms. Grounded in the psychological literature, we proposed LLM Implicit Bias to measure implicit biases and found prevalent stereotype biases in a set of value-aligned LLMs across diverse social categories, many of which reflect existing stereotypes that divide human society. These implicit biases are diagnostic of model behaviors in decision tasks as measured by our LLM Decision Bias, demonstrating their importance. Taken together, our approach showcases how psychology can inspire new approaches for assessing LLMs.
:::
:::
